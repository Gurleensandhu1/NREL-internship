Failure # 1 (occurred at 2021-06-24_21-37-19)
Traceback (most recent call last):
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 426, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 378, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/worker.py", line 1457, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(FatalTraCIError): [36mray::PPO.train()[39m (pid=137554, ip=10.148.4.97)
  File "python/ray/_raylet.pyx", line 636, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 619, in ray._raylet.execute_task.function_executor
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 444, in train
    raise e
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 433, in train
    result = Trainable.train(self)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/trainable.py", line 176, in train
    result = self._train()
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 129, in _train
    fetches = self.optimizer.step()
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py", line 140, in step
    self.num_envs_per_worker, self.train_batch_size)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/optimizers/rollout.py", line 29, in collect_samples
    next_sample = ray_get_and_free(fut_sample)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/utils/memory.py", line 33, in ray_get_and_free
    result = ray.get(object_ids)
ray.exceptions.RayTaskError(FatalTraCIError): [36mray::RolloutWorker.sample()[39m (pid=137550, ip=10.148.4.97)
  File "python/ray/_raylet.pyx", line 636, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 619, in ray._raylet.execute_task.function_executor
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 471, in sample
    batches = [self.input_reader.next()]
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 56, in next
    batches = [self.get_data()]
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 99, in get_data
    item = next(self.rollout_provider)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 319, in _env_runner
    soft_horizon, no_done_at_end)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 480, in _process_observations
    resetted_obs = base_env.try_reset(env_id)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/env/base_env.py", line 336, in try_reset
    return {_DUMMY_AGENT_ID: self.vector_env.reset_at(env_id)}
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/env/vector_env.py", line 104, in reset_at
    return self.envs[index].reset()
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/cavs-central-control/refactor/configs/merge/environment.py", line 369, in reset
    return super().reset()
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/flow/flow/envs/base.py", line 465, in reset
    self.restart_simulation(self.sim_params)
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/flow/flow/envs/base.py", line 264, in restart_simulation
    network=self.k.network, sim_params=self.sim_params)
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/flow/flow/core/kernel/simulation/traci.py", line 260, in start_simulation
    raise error
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/flow/flow/core/kernel/simulation/traci.py", line 252, in start_simulation
    traci_connection.setOrder(0)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/traci/connection.py", line 349, in setOrder
    self._sendExact()
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/traci/connection.py", line 100, in _sendExact
    raise FatalTraCIError("connection closed by SUMO")
traci.exceptions.FatalTraCIError: connection closed by SUMO

Failure # 2 (occurred at 2021-06-24_21-43-43)
Traceback (most recent call last):
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 426, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 378, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/worker.py", line 1457, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(FatalTraCIError): [36mray::PPO.train()[39m (pid=141522, ip=10.148.4.97)
  File "python/ray/_raylet.pyx", line 636, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 619, in ray._raylet.execute_task.function_executor
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 444, in train
    raise e
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 433, in train
    result = Trainable.train(self)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/trainable.py", line 176, in train
    result = self._train()
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 129, in _train
    fetches = self.optimizer.step()
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py", line 140, in step
    self.num_envs_per_worker, self.train_batch_size)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/optimizers/rollout.py", line 29, in collect_samples
    next_sample = ray_get_and_free(fut_sample)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/utils/memory.py", line 33, in ray_get_and_free
    result = ray.get(object_ids)
ray.exceptions.RayTaskError(FatalTraCIError): [36mray::RolloutWorker.sample()[39m (pid=141689, ip=10.148.4.97)
  File "python/ray/_raylet.pyx", line 636, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 619, in ray._raylet.execute_task.function_executor
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 471, in sample
    batches = [self.input_reader.next()]
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 56, in next
    batches = [self.get_data()]
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 99, in get_data
    item = next(self.rollout_provider)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 319, in _env_runner
    soft_horizon, no_done_at_end)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 480, in _process_observations
    resetted_obs = base_env.try_reset(env_id)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/env/base_env.py", line 336, in try_reset
    return {_DUMMY_AGENT_ID: self.vector_env.reset_at(env_id)}
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/env/vector_env.py", line 104, in reset_at
    return self.envs[index].reset()
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/cavs-central-control/refactor/configs/merge/environment.py", line 369, in reset
    return super().reset()
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/flow/flow/envs/base.py", line 465, in reset
    self.restart_simulation(self.sim_params)
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/flow/flow/envs/base.py", line 264, in restart_simulation
    network=self.k.network, sim_params=self.sim_params)
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/flow/flow/core/kernel/simulation/traci.py", line 260, in start_simulation
    raise error
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/flow/flow/core/kernel/simulation/traci.py", line 252, in start_simulation
    traci_connection.setOrder(0)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/traci/connection.py", line 349, in setOrder
    self._sendExact()
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/traci/connection.py", line 100, in _sendExact
    raise FatalTraCIError("connection closed by SUMO")
traci.exceptions.FatalTraCIError: connection closed by SUMO

Failure # 3 (occurred at 2021-06-24_22-47-44)
Traceback (most recent call last):
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 426, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 378, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/worker.py", line 1457, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OSError): [36mray::PPO.train()[39m (pid=143392, ip=10.148.4.97)
  File "python/ray/_raylet.pyx", line 636, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 619, in ray._raylet.execute_task.function_executor
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 444, in train
    raise e
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 433, in train
    result = Trainable.train(self)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/trainable.py", line 176, in train
    result = self._train()
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 129, in _train
    fetches = self.optimizer.step()
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py", line 140, in step
    self.num_envs_per_worker, self.train_batch_size)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/optimizers/rollout.py", line 29, in collect_samples
    next_sample = ray_get_and_free(fut_sample)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/utils/memory.py", line 33, in ray_get_and_free
    result = ray.get(object_ids)
ray.exceptions.RayTaskError(OSError): [36mray::RolloutWorker.sample()[39m (pid=143397, ip=10.148.4.97)
  File "python/ray/_raylet.pyx", line 636, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 619, in ray._raylet.execute_task.function_executor
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 471, in sample
    batches = [self.input_reader.next()]
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 56, in next
    batches = [self.get_data()]
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 99, in get_data
    item = next(self.rollout_provider)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 319, in _env_runner
    soft_horizon, no_done_at_end)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 480, in _process_observations
    resetted_obs = base_env.try_reset(env_id)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/env/base_env.py", line 336, in try_reset
    return {_DUMMY_AGENT_ID: self.vector_env.reset_at(env_id)}
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/env/vector_env.py", line 104, in reset_at
    return self.envs[index].reset()
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/cavs-central-control/refactor/configs/merge/environment.py", line 337, in reset
    print("=============================== ENV:reset ======================")
OSError: [Errno 28] No space left on device

Failure # 4 (occurred at 2021-06-24_22-49-14)
Traceback (most recent call last):
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 426, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 378, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/worker.py", line 1457, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OSError): [36mray::PPO.train()[39m (pid=143392, ip=10.148.4.97)
  File "python/ray/_raylet.pyx", line 636, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 619, in ray._raylet.execute_task.function_executor
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 444, in train
    raise e
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 433, in train
    result = Trainable.train(self)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/trainable.py", line 176, in train
    result = self._train()
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 129, in _train
    fetches = self.optimizer.step()
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py", line 140, in step
    self.num_envs_per_worker, self.train_batch_size)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/optimizers/rollout.py", line 29, in collect_samples
    next_sample = ray_get_and_free(fut_sample)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/utils/memory.py", line 33, in ray_get_and_free
    result = ray.get(object_ids)
ray.exceptions.RayTaskError(OSError): [36mray::RolloutWorker.sample()[39m (pid=143397, ip=10.148.4.97)
  File "python/ray/_raylet.pyx", line 636, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 619, in ray._raylet.execute_task.function_executor
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 471, in sample
    batches = [self.input_reader.next()]
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 56, in next
    batches = [self.get_data()]
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 99, in get_data
    item = next(self.rollout_provider)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 319, in _env_runner
    soft_horizon, no_done_at_end)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 480, in _process_observations
    resetted_obs = base_env.try_reset(env_id)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/env/base_env.py", line 336, in try_reset
    return {_DUMMY_AGENT_ID: self.vector_env.reset_at(env_id)}
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/rllib/env/vector_env.py", line 104, in reset_at
    return self.envs[index].reset()
  File "/lustre/eaglefs/projects/cavs/mlunacek/flow_work/cavs-central-control/refactor/configs/merge/environment.py", line 337, in reset
    print("=============================== ENV:reset ======================")
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 222, in start_trial
    self._start_trial(trial, checkpoint, remote_runner)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 156, in _start_trial
    self.restore(trial, checkpoint)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 608, in restore
    DEFAULT_GET_TIMEOUT)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/worker.py", line 1450, in get
    values = worker.get_objects(object_ids, timeout=timeout)
  File "/projects/aces/mlunacek/condaenvs/flow/lib/python3.7/site-packages/ray/worker.py", line 318, in get_objects
    object_ids, self.current_task_id, timeout_ms)
  File "python/ray/_raylet.pyx", line 815, in ray._raylet.CoreWorker.get_objects
  File "python/ray/_raylet.pyx", line 169, in ray._raylet.check_status
ray.exceptions.RayTimeoutError: Get timed out: some object(s) not ready.

